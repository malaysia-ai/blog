{
  
    
        "post0": {
            "title": "Virtual Background",
            "content": "Since PKP lot of people start working remotely and app like Zoom let you do virtual background without green screen for privacy to hide whatever at your back. . What is Virtual Background . Virtual Backgrounds is a feature that replaces the background with another image, mainly when you don’t want the other person to see your room. Among the video conferencing software out there, Zoom, MS Teams, Amazon Chime support Virtual backgrounds, Agora.io and Twillio don’t seem to offer them. . Diagram . Code . !pip install mediapipe -q . !wget &quot;https://img.freepik.com/free-photo/green-park-view_1417-1492.jpg?size=626&amp;ext=jpg&quot; -O ./artifacts/bg.jpg . --2021-10-31 16:24:35-- https://img.freepik.com/free-photo/green-park-view_1417-1492.jpg?size=626&amp;ext=jpg Resolving img.freepik.com (img.freepik.com)... 2600:140e:6:bbf::30ec, 2600:140e:6:b8d::30ec, 184.31.29.148 Connecting to img.freepik.com (img.freepik.com)|2600:140e:6:bbf::30ec|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 113518 (111K) [image/jpeg] Saving to: ‘./artifacts/bg.jpg’ ./artifacts/bg.jpg 100%[===================&gt;] 110.86K --.-KB/s in 0.1s 2021-10-31 16:24:36 (896 KB/s) - ‘./artifacts/bg.jpg’ saved [113518/113518] . import cv2 import mediapipe as mp import numpy as np mp_drawing = mp.solutions.drawing_utils mp_selfie_segmentation = mp.solutions.selfie_segmentation from fastai.basics import * . selfie_segmentation = mp_selfie_segmentation.SelfieSegmentation(model_selection=0) selfie_segmentation . INFO: Created TensorFlow Lite XNNPACK delegate for CPU. . &lt;mediapipe.python.solutions.selfie_segmentation.SelfieSegmentation at 0x7fabe9baff10&gt; . file = &quot;./artifacts/ben_afflek/httpimagesfandangocomrImageRendererredesignstaticimgnoxportraitjpgpcpcpcimagesmasterrepositoryperformerimagespjpg.jpg&quot; image = cv2.imread(file) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) image_height, image_width, _ = image.shape plt.imshow(image) . &lt;matplotlib.image.AxesImage at 0x7fabf150f9a0&gt; . results = selfie_segmentation.process(image)# Convert the BGR image to RGB before processing. plt.imshow(results.segmentation_mask) . &lt;matplotlib.image.AxesImage at 0x7fabf1a25190&gt; . fg_image = cv2.imread(file) fg_image = cv2.cvtColor(fg_image, cv2.COLOR_BGR2RGB) x,y,c = fg_image.shape x,y,c . (250, 164, 3) . condition = np.stack((results.segmentation_mask,) * 3, axis=-1) &gt; 0.1 # bilateral filter plt.imshow(np.where(condition, fg_image, np.zeros(image.shape, dtype=np.uint8))) . &lt;matplotlib.image.AxesImage at 0x7fabf1bc3220&gt; . bg_image = cv2.imread(&quot;./artifacts/bg.jpg&quot;) bg_image = bg_image[:x,:y] bg_image.shape plt.imshow(bg_image) . &lt;matplotlib.image.AxesImage at 0x7fabf1e21b50&gt; . output_image = np.where(condition, fg_image, bg_image) plt.imshow(output_image) . &lt;matplotlib.image.AxesImage at 0x7fabf1f13850&gt; . Conclusion . Virtual background is a simple technique that you only need to know how to extract the foreground and background and combine them as final result . Reference: . https://google.github.io/mediapipe/solutions/selfie_segmentation |",
            "url": "https://blog.malaysiaai.ml/2021/10/31/virtual_background.html",
            "relUrl": "/2021/10/31/virtual_background.html",
            "date": " • Oct 31, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Face Verification",
            "content": "I was planning to write this post since early October but because of work and I need to move to another company this post take quite some time. Enough excuse :p this post gonna talk about face verification usually use for verification. . What is Face Verification . You’re sitting at home. You want to apply for a visa for an upcoming vacation. You open your laptop, or pick up your mobile phone, and log on to the government visa service. You use the device camera to scan your driver’s license or passport to prove your identity. You then scan your face. Facial verification technology confirms that your physical face matches the one in the ID document, and that you are real and completing this application right now. -- iproov . Based on above statement we need two image, one from your driver&#39;s licence/passport another one is your selfie image so that the system can compare. . Diagram . Code . !pip install facenet-pytorch -q . from facenet_pytorch import MTCNN, InceptionResnetV1 import matplotlib.pyplot as plt import numpy as np from fastai.vision.all import show_image from PIL import Image . size = (200,200) . mtcnn = MTCNN( image_size=160, margin=0, min_face_size=1, thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True, # device=device ) . resnet = InceptionResnetV1(pretrained=&#39;vggface2&#39;).eval()#.to(device) . def get_embedding(path,show=True): img = Image.open(path) if show: display(img.resize(size)) img_cropped = mtcnn(img) if show: show_image(unorm(img_cropped)) # Calculate embedding (unsqueeze to add batch dimension) img_embedding = resnet(img_cropped.unsqueeze(0)) return img_embedding[0] . embeddingA = get_embedding(&quot;./artifacts/ben_afflek/httpcsvkmeuaeccjpg.jpg&quot;) embeddingA.shape . torch.Size([512]) . embeddingB = get_embedding(&quot;./artifacts/ben_afflek/httpimagesfandangocomrImageRendererredesignstaticimgnoxportraitjpgpcpcpcimagesmasterrepositoryperformerimagespjpg.jpg&quot;) embeddingB.shape . torch.Size([512]) . (embeddingA - embeddingB).norm() . tensor(0.8929, grad_fn=&lt;CopyBackwards&gt;) . Above example calculate the distance between two same person, let&#39;s try with two different people . embeddingB = get_embedding(&quot;./artifacts/elton_john/httpftqncomymusicLxZeltonjohnjpg.jpg&quot;) embeddingB.shape . torch.Size([512]) . (embeddingA - embeddingB).norm() . tensor(1.4721, grad_fn=&lt;CopyBackwards&gt;) . As you can see above Ben afflek and Elton John is two different person, their distance will be high . Conclusion . Human do face recognition by comparing feature of a person that is stored in their memory and compare with the person in front of them. That&#39;s why we did not do similar technique being done like Image classification for Imagenet dataset for example. With comparing we don&#39;t need to have fixed number of class . Glossary . Embedding = Vector consist of the image information that is learned by the model(similar image will be near to each other). You can think of embedding like array of one dimension [1,2] and [1,3]. If you plot them they will be near to each other compare to [10,200].Usually embedding got lot more column like 256 and not two column(x and y) like above which make it more expressive. | Distance = measure of how much alike two data objects are https://dataaspirant.com/five-most-popular-similarity-measures-implementation-in-python/ | Reference: . https://www.iproov.com/blog/face-recognition-face-verification-whats-the-difference | https://www.kaggle.com/dansbecker/5-celebrity-faces-dataset | https://github.com/timesler/facenet-pytorch/blob/master/examples/infer.ipynb |",
            "url": "https://blog.malaysiaai.ml/2021/10/30/face_verification.html",
            "relUrl": "/2021/10/30/face_verification.html",
            "date": " • Oct 30, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Speaker Diarization with Zero shot Speaker Identification",
            "content": "We will look into a simple speech pipeline to do speaker identification for speaker diarization, and ElasticSearch Malaysia-AI as vector database to store speaker vectors. . . Install necessary libraries . For speech toolkit, I am going to use https://malaya-speech.readthedocs.io to help me for Speaker Diarization and Speaker Identification. . pip install malaya-speech elasticsearch pycookiecheat requests . Make sure we are connected to ElasticSearch Malaysia-AI . If you are part of Malaysia-AI, you can query ElasticSearch Malaysia-AI from anywhere! . Before that, we must login to https://elasticsearch.malaysiaai.ml using Chrome first. . from pycookiecheat import chrome_cookies from elasticsearch import Elasticsearch import requests . url = &#39;https://elasticsearch.malaysiaai.ml&#39; cookies = chrome_cookies(url) . requests.get(url, cookies = cookies).json() . {&#39;name&#39;: &#39;malaysia-ai&#39;, &#39;cluster_name&#39;: &#39;elasticsearch&#39;, &#39;cluster_uuid&#39;: &#39;B2YuhAj3T9i2KXnBbAc1uA&#39;, &#39;version&#39;: {&#39;number&#39;: &#39;7.15.0&#39;, &#39;build_flavor&#39;: &#39;default&#39;, &#39;build_type&#39;: &#39;deb&#39;, &#39;build_hash&#39;: &#39;79d65f6e357953a5b3cbcc5e2c7c21073d89aa29&#39;, &#39;build_date&#39;: &#39;2021-09-16T03:05:29.143308416Z&#39;, &#39;build_snapshot&#39;: False, &#39;lucene_version&#39;: &#39;8.9.0&#39;, &#39;minimum_wire_compatibility_version&#39;: &#39;6.8.0&#39;, &#39;minimum_index_compatibility_version&#39;: &#39;6.0.0-beta1&#39;}, &#39;tagline&#39;: &#39;You Know, for Search&#39;} . cookie_string = [] for k, v in cookies.items(): cookie_string.append(f&#39;{k}={v}&#39;) cookie_string = &#39;; &#39;.join(cookie_string) es = Elasticsearch(hosts = [url], headers = {&#39;Cookie&#39;: cookie_string}) resp = es.info() resp . {&#39;name&#39;: &#39;malaysia-ai&#39;, &#39;cluster_name&#39;: &#39;elasticsearch&#39;, &#39;cluster_uuid&#39;: &#39;B2YuhAj3T9i2KXnBbAc1uA&#39;, &#39;version&#39;: {&#39;number&#39;: &#39;7.15.0&#39;, &#39;build_flavor&#39;: &#39;default&#39;, &#39;build_type&#39;: &#39;deb&#39;, &#39;build_hash&#39;: &#39;79d65f6e357953a5b3cbcc5e2c7c21073d89aa29&#39;, &#39;build_date&#39;: &#39;2021-09-16T03:05:29.143308416Z&#39;, &#39;build_snapshot&#39;: False, &#39;lucene_version&#39;: &#39;8.9.0&#39;, &#39;minimum_wire_compatibility_version&#39;: &#39;6.8.0&#39;, &#39;minimum_index_compatibility_version&#39;: &#39;6.0.0-beta1&#39;}, &#39;tagline&#39;: &#39;You Know, for Search&#39;} . Good to go! . Load example speakers . I have a few audios, each presented a unique speaker. . import malaya_speech import matplotlib.pyplot as plt from glob import glob import os audios = glob(&#39;artifacts/*.wav&#39;) audios . [&#39;artifacts/haqkiem.wav&#39;, &#39;artifacts/khalil-nooh.wav&#39;, &#39;artifacts/mas-aisyah.wav&#39;, &#39;artifacts/female.wav&#39;, &#39;artifacts/shafiqah-idayu.wav&#39;, &#39;artifacts/husein-zolkepli.wav&#39;] . os.path.split(audios[0])[1] . &#39;haqkiem.wav&#39; . sr = 16000 Y, speakers = [], [] for a in audios: speaker = os.path.split(a)[1].replace(&#39;.wav&#39;, &#39;&#39;) y, _ = malaya_speech.load(a, sr = sr) Y.append(y) speakers.append(speaker) . Convert audios into represent vector . We can use any speaker vector models available on the internet, but in this notebook, I am going to use model from https://malaya-speech.readthedocs.io/en/latest/load-speaker-vector.html . malaya_speech.speaker_vector.available_model() . INFO:root:tested on VoxCeleb2 test set. Lower EER is better. . Size (MB) Quantized Size (MB) Embedding Size EER . deep-speaker 96.7 | 24.40 | 512.0 | 0.21870 | . vggvox-v1 70.8 | 17.70 | 1024.0 | 0.14070 | . vggvox-v2 43.2 | 7.92 | 512.0 | 0.04450 | . speakernet 35.0 | 8.88 | 7205.0 | 0.02122 | . I am going to load vggvox-v2 model, embedding size just 512, nice to store in a small ElasticSearch. . model = malaya_speech.speaker_vector.deep_model(&#39;vggvox-v2&#39;) . INFO:root:running speaker-vector/vggvox-v2 using device /device:CPU:0 . Let&#39;s we visualize one sample audio, . plt.plot(Y[0]) plt.show() . And the length of this sample is, . len(Y[0]) / 16000 . 8.2314375 . 8 seconds, pretty long to feed into speaker vector model. Yes, we can feed directly to the model, but it is better we chunk the sample into multiple samples to get different speaker vectors. To chunk it, we can use Voice Activity Detection module to only sample positive voice activities. . Chunks using VAD . I am going to use WebRTC for our VAD model, malaya-speech also provided deep learning model for better VAD, https://malaya-speech.readthedocs.io/en/latest/load-vad.html, but for this example, we keep it simple. . pip install webrtcvad . malaya-speech provided simple interface for webRTC. . vad = malaya_speech.vad.webrtc() . How to detect Voice Activity . In order to use available Malaya-Speech VAD models, we need to split our audio sample into really small chunks. . For Google WebRTC, we need to split by every 30 ms and the frame must be in integer format. For deep learning, we trained on 30 ms, 90 ms and random length less than 300 ms. . To split an audio sample by giving time split, use, malaya_speech.utils.generator.frames. . from malaya_speech import Pipeline p = Pipeline() to_int = p.map(malaya_speech.utils.astype.float_to_int) int_frames = to_int.map(malaya_speech.utils.generator.frames, append_ending_trail = False) float_frames = p.map(malaya_speech.utils.generator.frames, append_ending_trail = False) vad_map = int_frames.foreach_map(vad) foreach = float_frames.foreach_zip(vad_map) foreach.map(malaya_speech.utils.group.group_frames) .map(malaya_speech.utils.group.group_frames_threshold, threshold_to_stop = 0.1) p.visualize() . malaya-speech provided a simple pipeline to combine multiple callables into a single pipeline, https://malaya-speech.readthedocs.io/en/latest/load-pipeline.html . The pipeline is simple, . audio -&gt; cast float to integer -&gt; split the audio into 30 ms small chunks -&gt; for each 30 ms small chunks will feed into WebRTC -&gt; combine result -&gt; group similar VAD labels into 1 longer sample -&gt; merged multiple different VAD labels if threshold below 0.05 seconds. . %%time result = p(Y[0]) result.keys() . CPU times: user 30.5 ms, sys: 2.93 ms, total: 33.4 ms Wall time: 32.2 ms . dict_keys([&#39;float_to_int&#39;, &#39;frames&#39;, &#39;vad&#39;, &#39;foreach_zip&#39;, &#39;group_frames&#39;, &#39;group_frames_threshold&#39;]) . result[&#39;group_frames_threshold&#39;][:5] . [(&lt;malaya_speech.model.frame.Frame at 0x14d1180d0&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d118090&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d118050&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d101f90&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d101fd0&gt;, True)] . malaya_speech.extra.visualization.visualize_vad(Y[0], result[&#39;group_frames_threshold&#39;], sr) . import IPython.display as ipd ipd.Audio(result[&#39;group_frames_threshold&#39;][0][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result[&#39;group_frames_threshold&#39;][1][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result[&#39;group_frames_threshold&#39;][2][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result[&#39;group_frames_threshold&#39;][3][0].array, rate = sr) . Your browser does not support the audio element. Apply VAD after that feed into speaker models . vectors = [] for i in range(len(Y)): result = p(Y[i]) for r in result[&#39;group_frames_threshold&#39;]: if r[1]: vectors.append((model([r[0]])[0], speakers[i])) len(vectors) . 25 . vectors, speakers = list(zip(*vectors)) . Visualize in lower dimension . from sklearn.decomposition import PCA pca = PCA(n_components = 2) components = pca.fit_transform(vectors) . fig, ax = plt.subplots() ax.scatter(components[:, 0], components[:, 1]) for i, speaker in enumerate(speakers): ax.annotate(speaker, (components[i, 0], components[i, 1])) plt.xlabel(&#39;Principal Component 1&#39;) plt.ylabel(&#39;Principal Component 2&#39;) plt.show() . Insert vectors into ElasticSearch . First, we need to define index mapping about the vectors, . MAPPING_DEALS = { &#39;mappings&#39;: { &#39;properties&#39;: { &#39;vector&#39;: {&#39;type&#39;: &#39;dense_vector&#39;, &#39;dims&#39;: vectors[0].shape[0]}, } }, } . index = &#39;test-speaker&#39; es.indices.delete(index = index, ignore = [400, 404]) es.indices.create(index = index, body = MAPPING_DEALS) . INFO:elasticsearch:DELETE https://elasticsearch.malaysiaai.ml:443/test-speaker [status:200 request:0.094s] INFO:elasticsearch:PUT https://elasticsearch.malaysiaai.ml:443/test-speaker [status:200 request:0.178s] . {&#39;acknowledged&#39;: True, &#39;shards_acknowledged&#39;: True, &#39;index&#39;: &#39;test-speaker&#39;} . from tqdm import tqdm # to speed up, you might want to use bulk insert for i in tqdm(range(len(vectors))): es.index(index = index, doc_type = &#39;_doc&#39;, body= {&#39;vector&#39;: vectors[i].tolist(), &#39;speaker&#39;: speakers[i]}) . 0%| | 0/25 [00:00&lt;?, ?it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.106s] 4%|▍ | 1/25 [00:00&lt;00:02, 9.26it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 16%|█▌ | 4/25 [00:00&lt;00:01, 11.11it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.042s] 28%|██▊ | 7/25 [00:00&lt;00:01, 12.99it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.043s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 40%|████ | 10/25 [00:00&lt;00:01, 14.73it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 52%|█████▏ | 13/25 [00:00&lt;00:00, 16.23it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 64%|██████▍ | 16/25 [00:00&lt;00:00, 17.54it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] 76%|███████▌ | 19/25 [00:00&lt;00:00, 18.50it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 88%|████████▊ | 22/25 [00:01&lt;00:00, 19.19it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] 100%|██████████| 25/25 [00:01&lt;00:00, 20.14it/s] . Sort based on cosine similarity from ElasticSearch . ElasticSearch support cosine similarity sorting, read more at https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch . data = { &#39;query&#39;: { &#39;script_score&#39;: { &#39;query&#39;: {&#39;match_all&#39;: {}}, &#39;script&#39;: { &#39;source&#39;: &quot;(cosineSimilarity(params.query_vector, &#39;vector&#39;))&quot;, &#39;params&#39;: {&#39;query_vector&#39;: vectors[0].tolist()}, }, } }, &#39;size&#39;: 2, &#39;_source&#39;: [&#39;speaker&#39;] } url_index = f&#39;{url}/{index}/_search&#39; r = requests.post(url_index, json = data, cookies = cookies).json() r . {&#39;took&#39;: 13, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 1, &#39;successful&#39;: 1, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 17, &#39;relation&#39;: &#39;eq&#39;}, &#39;max_score&#39;: 0.99999994, &#39;hits&#39;: [{&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;p8yrJ3wBXDIj-_AAmIBw&#39;, &#39;_score&#39;: 0.99999994, &#39;_source&#39;: {&#39;speaker&#39;: &#39;haqkiem&#39;}}, {&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;r8yrJ3wBXDIj-_AAmoAU&#39;, &#39;_score&#39;: 0.82846445, &#39;_source&#39;: {&#39;speaker&#39;: &#39;haqkiem&#39;}}]}} . Now, speaker diarization . Let say I have an audio with multiple speakers in it, I want to diarize it and identify each speakers, to read more about speaker diarization at https://malaya-speech.readthedocs.io/en/latest/load-diarization.html . from pydub import AudioSegment import numpy as np audio = AudioSegment.from_file(&#39;artifacts/husein-ayu.m4a&#39;) samples = np.array(audio.get_array_of_samples()) samples = malaya_speech.astype.int_to_float(samples) samples = malaya_speech.resample(samples, audio.frame_rate, 16000) . audio . Your browser does not support the audio element. plt.plot(samples) . [&lt;matplotlib.lines.Line2D at 0x15ca851d0&gt;] . To do speaker diarization, we need to VAD -&gt; group VAD -&gt; Populate positive VAD -&gt; clustering. . %%time result = p(samples) result.keys() . CPU times: user 196 ms, sys: 27.3 ms, total: 224 ms Wall time: 82.2 ms . dict_keys([&#39;float_to_int&#39;, &#39;frames&#39;, &#39;vad&#39;, &#39;foreach_zip&#39;, &#39;group_frames&#39;, &#39;group_frames_threshold&#39;]) . malaya_speech.extra.visualization.visualize_vad(samples, result[&#39;group_frames_threshold&#39;], sr) . result_diarization_sc = malaya_speech.diarization.spectral_cluster(result[&#39;group_frames_threshold&#39;], model, min_clusters = 2, max_clusters = 100) result_diarization_sc[:20] . [(&lt;malaya_speech.model.frame.Frame at 0x14e0ea610&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0ea350&gt;, &#39;speaker 2&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0ea650&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x15c9ecc10&gt;, &#39;speaker 2&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7c50&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7510&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7190&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7f90&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f70d0&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f72d0&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7290&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f75d0&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fab90&gt;, &#39;speaker 1&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa7d0&gt;, &#39;speaker 3&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa990&gt;, &#39;speaker 1&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa750&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa4d0&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa5d0&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa890&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa8d0&gt;, &#39;not a speaker&#39;)] . nrows = 2 fig, ax = plt.subplots(nrows = nrows, ncols = 1) fig.set_figwidth(20) fig.set_figheight(nrows * 3) malaya_speech.extra.visualization.visualize_vad(samples, result[&#39;group_frames_threshold&#39;], sr, ax = ax[0]) malaya_speech.extra.visualization.plot_classification(result_diarization_sc, &#39;diarization using spectral cluster&#39;, ax = ax[1], x_text = 0.01) fig.tight_layout() plt.show() . ipd.Audio(result_diarization_sc[1][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result_diarization_sc[3][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result_diarization_sc[7][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result_diarization_sc[12][0].array, rate = sr) . Your browser does not support the audio element. Now, who is the speaker for result_diarization_sc[1]? . v = model([result_diarization_sc[1][0].array])[0] data = { &#39;query&#39;: { &#39;script_score&#39;: { &#39;query&#39;: {&#39;match_all&#39;: {}}, &#39;script&#39;: { &#39;source&#39;: &quot;(cosineSimilarity(params.query_vector, &#39;vector&#39;))&quot;, &#39;params&#39;: {&#39;query_vector&#39;: v.tolist()}, }, } }, &#39;size&#39;: 1, &#39;_source&#39;: [&#39;speaker&#39;] } url_index = f&#39;{url}/{index}/_search&#39; r = requests.post(url_index, json = data, cookies = cookies).json() r . {&#39;took&#39;: 2, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 1, &#39;successful&#39;: 1, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 25, &#39;relation&#39;: &#39;eq&#39;}, &#39;max_score&#39;: 0.8808331, &#39;hits&#39;: [{&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;vsyrJ3wBXDIj-_AAnIDX&#39;, &#39;_score&#39;: 0.8808331, &#39;_source&#39;: {&#39;speaker&#39;: &#39;husein-zolkepli&#39;}}]}} . Now, who is the speaker for result_diarization_sc[12]? . v = model([result_diarization_sc[12][0].array])[0] data = { &#39;query&#39;: { &#39;script_score&#39;: { &#39;query&#39;: {&#39;match_all&#39;: {}}, &#39;script&#39;: { &#39;source&#39;: &quot;(cosineSimilarity(params.query_vector, &#39;vector&#39;))&quot;, &#39;params&#39;: {&#39;query_vector&#39;: v.tolist()}, }, } }, &#39;size&#39;: 1, &#39;_source&#39;: [&#39;speaker&#39;] } url_index = f&#39;{url}/{index}/_search&#39; r = requests.post(url_index, json = data, cookies = cookies).json() r . {&#39;took&#39;: 2, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 1, &#39;successful&#39;: 1, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 25, &#39;relation&#39;: &#39;eq&#39;}, &#39;max_score&#39;: 0.8343149, &#39;hits&#39;: [{&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;vcyrJ3wBXDIj-_AAnICn&#39;, &#39;_score&#39;: 0.8343149, &#39;_source&#39;: {&#39;speaker&#39;: &#39;shafiqah-idayu&#39;}}]}} .",
            "url": "https://blog.malaysiaai.ml/2021/09/27/speaker-diarization-zero-shot-speaker-identification.html",
            "relUrl": "/2021/09/27/speaker-diarization-zero-shot-speaker-identification.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Malaysia AI",
            "content": "Description . A non profit organization to gather open source artificial intelligence for Malaysia. https://github.com/malaysia-ai/malaysia-ai . Mission . Providing Dataset / Models and Resources (machine / GPU / TPU and tools) to anyone that will contribute to Malaysian society. . Vision . Reduce friction for Malaysian people to get started in AI. . Vision of vision . Make AI less foreign in this country. . . Current Project . malay-spacy, Spacy for Malay language. . | malay-huggingface, Compile Malay NLP models for HuggingFace. . | Knowledge-Graph-Neo4j, Wikipedia and local news semisupervised Knowledge Graph on Neo4j. If you part of the organization, you can access to the databases! . | Elasticsearch, Store general purpose for texts. If you part of the organization, you can access to EK! . | code-server, If you part of the organization, you can access to code-server! . | dataset, Curating dataset related to Malaysia from multiple domain such as Tabular, Image, Text and Audio. . | label-studio, Public Label Studio, for everyone! . | Jupyterhub, Jupyterhub with GPU. If you part of the organization, you can access to jupyterhub! . | blog, Malaysia-AI Blog. . | projects, Collecting projects related to Malaysia from multiple domain such as Tabular, Image, Text and Audio. . | zerotier, Easy VPN for Malaysia AI machines. Feel free to donate any machines for Malaysia AI! . | Demo . https://huggingface.co/malay-huggingface |",
            "url": "https://blog.malaysiaai.ml/markdown/2021/09/26/hello-world.html",
            "relUrl": "/markdown/2021/09/26/hello-world.html",
            "date": " • Sep 26, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "AI community in Malaysia to share models/datasets/resources for helping Malaysian get started in AI (Malaysia AI) .",
          "url": "https://blog.malaysiaai.ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.malaysiaai.ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}