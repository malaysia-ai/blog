{
  
    
        "post0": {
            "title": "Speaker Diarization with Zero shot Speaker Identification",
            "content": "We will look into a simple speech pipeline to do speaker identification for speaker diarization, and ElasticSearch Malaysia-AI as vector database to store speaker vectors. . . Install necessary libraries . For speech toolkit, I am going to use https://malaya-speech.readthedocs.io to help me for Speaker Diarization and Speaker Identification. . pip install malaya-speech elasticsearch pycookiecheat requests . Make sure we are connected to ElasticSearch Malaysia-AI . If you are part of Malaysia-AI, you can query ElasticSearch Malaysia-AI from anywhere! . Before that, we must login to https://elasticsearch.malaysiaai.ml using Chrome first. . from pycookiecheat import chrome_cookies from elasticsearch import Elasticsearch import requests . url = &#39;https://elasticsearch.malaysiaai.ml&#39; cookies = chrome_cookies(url) . requests.get(url, cookies = cookies).json() . {&#39;name&#39;: &#39;malaysia-ai&#39;, &#39;cluster_name&#39;: &#39;elasticsearch&#39;, &#39;cluster_uuid&#39;: &#39;B2YuhAj3T9i2KXnBbAc1uA&#39;, &#39;version&#39;: {&#39;number&#39;: &#39;7.15.0&#39;, &#39;build_flavor&#39;: &#39;default&#39;, &#39;build_type&#39;: &#39;deb&#39;, &#39;build_hash&#39;: &#39;79d65f6e357953a5b3cbcc5e2c7c21073d89aa29&#39;, &#39;build_date&#39;: &#39;2021-09-16T03:05:29.143308416Z&#39;, &#39;build_snapshot&#39;: False, &#39;lucene_version&#39;: &#39;8.9.0&#39;, &#39;minimum_wire_compatibility_version&#39;: &#39;6.8.0&#39;, &#39;minimum_index_compatibility_version&#39;: &#39;6.0.0-beta1&#39;}, &#39;tagline&#39;: &#39;You Know, for Search&#39;} . cookie_string = [] for k, v in cookies.items(): cookie_string.append(f&#39;{k}={v}&#39;) cookie_string = &#39;; &#39;.join(cookie_string) es = Elasticsearch(hosts = [url], headers = {&#39;Cookie&#39;: cookie_string}) resp = es.info() resp . {&#39;name&#39;: &#39;malaysia-ai&#39;, &#39;cluster_name&#39;: &#39;elasticsearch&#39;, &#39;cluster_uuid&#39;: &#39;B2YuhAj3T9i2KXnBbAc1uA&#39;, &#39;version&#39;: {&#39;number&#39;: &#39;7.15.0&#39;, &#39;build_flavor&#39;: &#39;default&#39;, &#39;build_type&#39;: &#39;deb&#39;, &#39;build_hash&#39;: &#39;79d65f6e357953a5b3cbcc5e2c7c21073d89aa29&#39;, &#39;build_date&#39;: &#39;2021-09-16T03:05:29.143308416Z&#39;, &#39;build_snapshot&#39;: False, &#39;lucene_version&#39;: &#39;8.9.0&#39;, &#39;minimum_wire_compatibility_version&#39;: &#39;6.8.0&#39;, &#39;minimum_index_compatibility_version&#39;: &#39;6.0.0-beta1&#39;}, &#39;tagline&#39;: &#39;You Know, for Search&#39;} . Good to go! . Load example speakers . I have a few audios, each presented a unique speaker. . import malaya_speech import matplotlib.pyplot as plt from glob import glob import os audios = glob(&#39;artifacts/*.wav&#39;) audios . [&#39;artifacts/haqkiem.wav&#39;, &#39;artifacts/khalil-nooh.wav&#39;, &#39;artifacts/mas-aisyah.wav&#39;, &#39;artifacts/female.wav&#39;, &#39;artifacts/shafiqah-idayu.wav&#39;, &#39;artifacts/husein-zolkepli.wav&#39;] . os.path.split(audios[0])[1] . &#39;haqkiem.wav&#39; . sr = 16000 Y, speakers = [], [] for a in audios: speaker = os.path.split(a)[1].replace(&#39;.wav&#39;, &#39;&#39;) y, _ = malaya_speech.load(a, sr = sr) Y.append(y) speakers.append(speaker) . Convert audios into represent vector . We can use any speaker vector models available on the internet, but in this notebook, I am going to use model from https://malaya-speech.readthedocs.io/en/latest/load-speaker-vector.html . malaya_speech.speaker_vector.available_model() . INFO:root:tested on VoxCeleb2 test set. Lower EER is better. . Size (MB) Quantized Size (MB) Embedding Size EER . deep-speaker 96.7 | 24.40 | 512.0 | 0.21870 | . vggvox-v1 70.8 | 17.70 | 1024.0 | 0.14070 | . vggvox-v2 43.2 | 7.92 | 512.0 | 0.04450 | . speakernet 35.0 | 8.88 | 7205.0 | 0.02122 | . I am going to load vggvox-v2 model, embedding size just 512, nice to store in a small ElasticSearch. . model = malaya_speech.speaker_vector.deep_model(&#39;vggvox-v2&#39;) . INFO:root:running speaker-vector/vggvox-v2 using device /device:CPU:0 . Let&#39;s we visualize one sample audio, . plt.plot(Y[0]) plt.show() . And the length of this sample is, . len(Y[0]) / 16000 . 8.2314375 . 8 seconds, pretty long to feed into speaker vector model. Yes, we can feed directly to the model, but it is better we chunk the sample into multiple samples to get different speaker vectors. To chunk it, we can use Voice Activity Detection module to only sample positive voice activities. . Chunks using VAD . I am going to use WebRTC for our VAD model, malaya-speech also provided deep learning model for better VAD, https://malaya-speech.readthedocs.io/en/latest/load-vad.html, but for this example, we keep it simple. . pip install webrtcvad . malaya-speech provided simple interface for webRTC. . vad = malaya_speech.vad.webrtc() . How to detect Voice Activity . In order to use available Malaya-Speech VAD models, we need to split our audio sample into really small chunks. . For Google WebRTC, we need to split by every 30 ms and the frame must be in integer format. For deep learning, we trained on 30 ms, 90 ms and random length less than 300 ms. . To split an audio sample by giving time split, use, malaya_speech.utils.generator.frames. . from malaya_speech import Pipeline p = Pipeline() to_int = p.map(malaya_speech.utils.astype.float_to_int) int_frames = to_int.map(malaya_speech.utils.generator.frames, append_ending_trail = False) float_frames = p.map(malaya_speech.utils.generator.frames, append_ending_trail = False) vad_map = int_frames.foreach_map(vad) foreach = float_frames.foreach_zip(vad_map) foreach.map(malaya_speech.utils.group.group_frames) .map(malaya_speech.utils.group.group_frames_threshold, threshold_to_stop = 0.1) p.visualize() . malaya-speech provided a simple pipeline to combine multiple callables into a single pipeline, https://malaya-speech.readthedocs.io/en/latest/load-pipeline.html . The pipeline is simple, . audio -&gt; cast float to integer -&gt; split the audio into 30 ms small chunks -&gt; for each 30 ms small chunks will feed into WebRTC -&gt; combine result -&gt; group similar VAD labels into 1 longer sample -&gt; merged multiple different VAD labels if threshold below 0.05 seconds. . %%time result = p(Y[0]) result.keys() . CPU times: user 30.5 ms, sys: 2.93 ms, total: 33.4 ms Wall time: 32.2 ms . dict_keys([&#39;float_to_int&#39;, &#39;frames&#39;, &#39;vad&#39;, &#39;foreach_zip&#39;, &#39;group_frames&#39;, &#39;group_frames_threshold&#39;]) . result[&#39;group_frames_threshold&#39;][:5] . [(&lt;malaya_speech.model.frame.Frame at 0x14d1180d0&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d118090&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d118050&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d101f90&gt;, True), (&lt;malaya_speech.model.frame.Frame at 0x14d101fd0&gt;, True)] . malaya_speech.extra.visualization.visualize_vad(Y[0], result[&#39;group_frames_threshold&#39;], sr) . import IPython.display as ipd ipd.Audio(result[&#39;group_frames_threshold&#39;][0][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result[&#39;group_frames_threshold&#39;][1][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result[&#39;group_frames_threshold&#39;][2][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result[&#39;group_frames_threshold&#39;][3][0].array, rate = sr) . Your browser does not support the audio element. Apply VAD after that feed into speaker models . vectors = [] for i in range(len(Y)): result = p(Y[i]) for r in result[&#39;group_frames_threshold&#39;]: if r[1]: vectors.append((model([r[0]])[0], speakers[i])) len(vectors) . 25 . vectors, speakers = list(zip(*vectors)) . Visualize in lower dimension . from sklearn.decomposition import PCA pca = PCA(n_components = 2) components = pca.fit_transform(vectors) . fig, ax = plt.subplots() ax.scatter(components[:, 0], components[:, 1]) for i, speaker in enumerate(speakers): ax.annotate(speaker, (components[i, 0], components[i, 1])) plt.xlabel(&#39;Principal Component 1&#39;) plt.ylabel(&#39;Principal Component 2&#39;) plt.show() . Insert vectors into ElasticSearch . First, we need to define index mapping about the vectors, . MAPPING_DEALS = { &#39;mappings&#39;: { &#39;properties&#39;: { &#39;vector&#39;: {&#39;type&#39;: &#39;dense_vector&#39;, &#39;dims&#39;: vectors[0].shape[0]}, } }, } . index = &#39;test-speaker&#39; es.indices.delete(index = index, ignore = [400, 404]) es.indices.create(index = index, body = MAPPING_DEALS) . INFO:elasticsearch:DELETE https://elasticsearch.malaysiaai.ml:443/test-speaker [status:200 request:0.094s] INFO:elasticsearch:PUT https://elasticsearch.malaysiaai.ml:443/test-speaker [status:200 request:0.178s] . {&#39;acknowledged&#39;: True, &#39;shards_acknowledged&#39;: True, &#39;index&#39;: &#39;test-speaker&#39;} . from tqdm import tqdm # to speed up, you might want to use bulk insert for i in tqdm(range(len(vectors))): es.index(index = index, doc_type = &#39;_doc&#39;, body= {&#39;vector&#39;: vectors[i].tolist(), &#39;speaker&#39;: speakers[i]}) . 0%| | 0/25 [00:00&lt;?, ?it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.106s] 4%|▍ | 1/25 [00:00&lt;00:02, 9.26it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 16%|█▌ | 4/25 [00:00&lt;00:01, 11.11it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.042s] 28%|██▊ | 7/25 [00:00&lt;00:01, 12.99it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.043s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 40%|████ | 10/25 [00:00&lt;00:01, 14.73it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 52%|█████▏ | 13/25 [00:00&lt;00:00, 16.23it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 64%|██████▍ | 16/25 [00:00&lt;00:00, 17.54it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.044s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] 76%|███████▌ | 19/25 [00:00&lt;00:00, 18.50it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.046s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] 88%|████████▊ | 22/25 [00:01&lt;00:00, 19.19it/s]INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.045s] INFO:elasticsearch:POST https://elasticsearch.malaysiaai.ml:443/test-speaker/_doc [status:201 request:0.047s] 100%|██████████| 25/25 [00:01&lt;00:00, 20.14it/s] . Sort based on cosine similarity from ElasticSearch . ElasticSearch support cosine similarity sorting, read more at https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch . data = { &#39;query&#39;: { &#39;script_score&#39;: { &#39;query&#39;: {&#39;match_all&#39;: {}}, &#39;script&#39;: { &#39;source&#39;: &quot;(cosineSimilarity(params.query_vector, &#39;vector&#39;))&quot;, &#39;params&#39;: {&#39;query_vector&#39;: vectors[0].tolist()}, }, } }, &#39;size&#39;: 2, &#39;_source&#39;: [&#39;speaker&#39;] } url_index = f&#39;{url}/{index}/_search&#39; r = requests.post(url_index, json = data, cookies = cookies).json() r . {&#39;took&#39;: 13, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 1, &#39;successful&#39;: 1, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 17, &#39;relation&#39;: &#39;eq&#39;}, &#39;max_score&#39;: 0.99999994, &#39;hits&#39;: [{&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;p8yrJ3wBXDIj-_AAmIBw&#39;, &#39;_score&#39;: 0.99999994, &#39;_source&#39;: {&#39;speaker&#39;: &#39;haqkiem&#39;}}, {&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;r8yrJ3wBXDIj-_AAmoAU&#39;, &#39;_score&#39;: 0.82846445, &#39;_source&#39;: {&#39;speaker&#39;: &#39;haqkiem&#39;}}]}} . Now, speaker diarization . Let say I have an audio with multiple speakers in it, I want to diarize it and identify each speakers, to read more about speaker diarization at https://malaya-speech.readthedocs.io/en/latest/load-diarization.html . from pydub import AudioSegment import numpy as np audio = AudioSegment.from_file(&#39;artifacts/husein-ayu.m4a&#39;) samples = np.array(audio.get_array_of_samples()) samples = malaya_speech.astype.int_to_float(samples) samples = malaya_speech.resample(samples, audio.frame_rate, 16000) . audio . Your browser does not support the audio element. plt.plot(samples) . [&lt;matplotlib.lines.Line2D at 0x15ca851d0&gt;] . To do speaker diarization, we need to VAD -&gt; group VAD -&gt; Populate positive VAD -&gt; clustering. . %%time result = p(samples) result.keys() . CPU times: user 196 ms, sys: 27.3 ms, total: 224 ms Wall time: 82.2 ms . dict_keys([&#39;float_to_int&#39;, &#39;frames&#39;, &#39;vad&#39;, &#39;foreach_zip&#39;, &#39;group_frames&#39;, &#39;group_frames_threshold&#39;]) . malaya_speech.extra.visualization.visualize_vad(samples, result[&#39;group_frames_threshold&#39;], sr) . result_diarization_sc = malaya_speech.diarization.spectral_cluster(result[&#39;group_frames_threshold&#39;], model, min_clusters = 2, max_clusters = 100) result_diarization_sc[:20] . [(&lt;malaya_speech.model.frame.Frame at 0x14e0ea610&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0ea350&gt;, &#39;speaker 2&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0ea650&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x15c9ecc10&gt;, &#39;speaker 2&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7c50&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7510&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7190&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7f90&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f70d0&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f72d0&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f7290&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0f75d0&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fab90&gt;, &#39;speaker 1&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa7d0&gt;, &#39;speaker 3&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa990&gt;, &#39;speaker 1&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa750&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa4d0&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa5d0&gt;, &#39;not a speaker&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa890&gt;, &#39;speaker 0&#39;), (&lt;malaya_speech.model.frame.Frame at 0x14e0fa8d0&gt;, &#39;not a speaker&#39;)] . nrows = 2 fig, ax = plt.subplots(nrows = nrows, ncols = 1) fig.set_figwidth(20) fig.set_figheight(nrows * 3) malaya_speech.extra.visualization.visualize_vad(samples, result[&#39;group_frames_threshold&#39;], sr, ax = ax[0]) malaya_speech.extra.visualization.plot_classification(result_diarization_sc, &#39;diarization using spectral cluster&#39;, ax = ax[1], x_text = 0.01) fig.tight_layout() plt.show() . ipd.Audio(result_diarization_sc[1][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result_diarization_sc[3][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result_diarization_sc[7][0].array, rate = sr) . Your browser does not support the audio element. ipd.Audio(result_diarization_sc[12][0].array, rate = sr) . Your browser does not support the audio element. Now, who is the speaker for result_diarization_sc[1]? . v = model([result_diarization_sc[1][0].array])[0] data = { &#39;query&#39;: { &#39;script_score&#39;: { &#39;query&#39;: {&#39;match_all&#39;: {}}, &#39;script&#39;: { &#39;source&#39;: &quot;(cosineSimilarity(params.query_vector, &#39;vector&#39;))&quot;, &#39;params&#39;: {&#39;query_vector&#39;: v.tolist()}, }, } }, &#39;size&#39;: 1, &#39;_source&#39;: [&#39;speaker&#39;] } url_index = f&#39;{url}/{index}/_search&#39; r = requests.post(url_index, json = data, cookies = cookies).json() r . {&#39;took&#39;: 2, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 1, &#39;successful&#39;: 1, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 25, &#39;relation&#39;: &#39;eq&#39;}, &#39;max_score&#39;: 0.8808331, &#39;hits&#39;: [{&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;vsyrJ3wBXDIj-_AAnIDX&#39;, &#39;_score&#39;: 0.8808331, &#39;_source&#39;: {&#39;speaker&#39;: &#39;husein-zolkepli&#39;}}]}} . Now, who is the speaker for result_diarization_sc[12]? . v = model([result_diarization_sc[12][0].array])[0] data = { &#39;query&#39;: { &#39;script_score&#39;: { &#39;query&#39;: {&#39;match_all&#39;: {}}, &#39;script&#39;: { &#39;source&#39;: &quot;(cosineSimilarity(params.query_vector, &#39;vector&#39;))&quot;, &#39;params&#39;: {&#39;query_vector&#39;: v.tolist()}, }, } }, &#39;size&#39;: 1, &#39;_source&#39;: [&#39;speaker&#39;] } url_index = f&#39;{url}/{index}/_search&#39; r = requests.post(url_index, json = data, cookies = cookies).json() r . {&#39;took&#39;: 2, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 1, &#39;successful&#39;: 1, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 25, &#39;relation&#39;: &#39;eq&#39;}, &#39;max_score&#39;: 0.8343149, &#39;hits&#39;: [{&#39;_index&#39;: &#39;test-speaker&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;vcyrJ3wBXDIj-_AAnICn&#39;, &#39;_score&#39;: 0.8343149, &#39;_source&#39;: {&#39;speaker&#39;: &#39;shafiqah-idayu&#39;}}]}} .",
            "url": "https://blog.malaysiaai.ml/2021/09/27/speaker-diarization-zero-shot-speaker-identification.html",
            "relUrl": "/2021/09/27/speaker-diarization-zero-shot-speaker-identification.html",
            "date": " • Sep 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Malaysia AI",
            "content": "Description . A non profit organization to gather open source artificial intelligence for Malaysia. https://github.com/malaysia-ai/malaysia-ai . Mission . Providing Dataset / Models and Resources (machine / GPU / TPU and tools) to anyone that will contribute to Malaysian society. . Vision . Reduce friction for Malaysian people to get started in AI. . Vision of vision . Make AI less foreign in this country. . . Current Project . malay-spacy, Spacy for Malay language. . | malay-huggingface, Compile Malay NLP models for HuggingFace. . | Knowledge-Graph-Neo4j, Wikipedia and local news semisupervised Knowledge Graph on Neo4j. If you part of the organization, you can access to the databases! . | Elasticsearch, Store general purpose for texts. If you part of the organization, you can access to EK! . | code-server, If you part of the organization, you can access to code-server! . | dataset, Curating dataset related to Malaysia from multiple domain such as Tabular, Image, Text and Audio. . | label-studio, Public Label Studio, for everyone! . | Jupyterhub, Jupyterhub with GPU. If you part of the organization, you can access to jupyterhub! . | blog, Malaysia-AI Blog. . | Demo . https://huggingface.co/malay-huggingface |",
            "url": "https://blog.malaysiaai.ml/markdown/2021/09/26/hello-world.html",
            "relUrl": "/markdown/2021/09/26/hello-world.html",
            "date": " • Sep 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://blog.malaysiaai.ml/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://blog.malaysiaai.ml/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "AI community in Malaysia to share models/datasets/resources for helping Malaysian get started in AI (Malaysia AI) .",
          "url": "https://blog.malaysiaai.ml/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.malaysiaai.ml/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}